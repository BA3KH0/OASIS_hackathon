{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.models as models\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import random_split\nfrom torch.utils.data import Subset","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:22:19.283001Z","iopub.execute_input":"2023-08-26T00:22:19.283754Z","iopub.status.idle":"2023-08-26T00:22:22.504953Z","shell.execute_reply.started":"2023-08-26T00:22:19.283716Z","shell.execute_reply":"2023-08-26T00:22:22.503972Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport time\nimport os\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:33:06.648941Z","iopub.execute_input":"2023-08-26T00:33:06.649316Z","iopub.status.idle":"2023-08-26T00:33:06.656886Z","shell.execute_reply.started":"2023-08-26T00:33:06.649284Z","shell.execute_reply":"2023-08-26T00:33:06.655802Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(f'Using {device} for inference')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:22:25.151034Z","iopub.execute_input":"2023-08-26T00:22:25.152056Z","iopub.status.idle":"2023-08-26T00:22:25.189618Z","shell.execute_reply.started":"2023-08-26T00:22:25.152017Z","shell.execute_reply":"2023-08-26T00:22:25.188569Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Using cuda for inference\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# 데이터 경로 및 하이퍼파라미터 설정\ndata_dir = '/kaggle/input/oasis-cnn-0825/data_for_CNN'  # 데이터 폴더 경로\nbatch_size = 32\nnum_classes = 3\nnum_epochs = 30\nlearning_rate = 0.001\n\n# 데이터 전처리\ntransform = transforms.Compose([\n    transforms.Resize((370, 370)),  # 이미지를 충분히 크게 리사이즈\n    transforms.CenterCrop(260),      # 중앙 부분을 224x224 크기로 자름\n    transforms.RandomRotation(degrees=15),  # 무작위 회전 (±15도)\n    transforms.RandomHorizontalFlip(),  # 무작위 수평 반전\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # 색상 변형\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nfull_dataset = ImageFolder(root=data_dir, transform=transform)\n\n# 클래스별로 Subset 생성\nclass_indices = [[] for _ in range(num_classes)]\nfor index, (image, label) in enumerate(full_dataset):\n    class_indices[label].append(index)\n\ntrain_indices = []\nval_indices = []\nfor class_idx in range(num_classes):\n    train_indices.extend(class_indices[class_idx][:-200])\n    val_indices.extend(class_indices[class_idx][-250:])\n\ntrain_dataset = Subset(full_dataset, train_indices)\nval_dataset = Subset(full_dataset, val_indices)\n\n# 데이터 로더 설정\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n\n# EfficientNet 모델 로드\nmodel = models.efficientnet_b2(pretrained=False, num_classes=num_classes).to(\"cuda\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:43:08.047534Z","iopub.execute_input":"2023-08-26T00:43:08.047914Z","iopub.status.idle":"2023-08-26T00:45:53.270755Z","shell.execute_reply.started":"2023-08-26T00:43:08.047883Z","shell.execute_reply":"2023-08-26T00:45:53.269121Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"learning_rate = 0.01","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:45:57.365510Z","iopub.execute_input":"2023-08-26T00:45:57.365876Z","iopub.status.idle":"2023-08-26T00:45:57.370956Z","shell.execute_reply.started":"2023-08-26T00:45:57.365844Z","shell.execute_reply":"2023-08-26T00:45:57.369735Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader)*32, len(val_dataloader)*32","metadata":{"execution":{"iopub.status.busy":"2023-08-25T06:31:54.246004Z","iopub.execute_input":"2023-08-25T06:31:54.246451Z","iopub.status.idle":"2023-08-25T06:31:54.254340Z","shell.execute_reply.started":"2023-08-25T06:31:54.246416Z","shell.execute_reply":"2023-08-25T06:31:54.252915Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"(9440, 608)"},"metadata":{}}]},{"cell_type":"code","source":"for images, labels in dataloader:\n    # images는 배치 내 이미지들의 텐서입니다.\n    # labels는 배치 내 이미지들에 해당하는 클래스 레이블입니다.\n    \n    # 이미지와 클래스를 확인하는 예제 코드\n    for i in range(len(images)):\n        image = images[i]\n        label = labels[i]\n        \n        # 이미지와 클래스 정보 출력\n        print(f\"Image shape: {image.shape}, Label: {label}\")","metadata":{"execution":{"iopub.status.busy":"2023-08-25T05:16:58.884691Z","iopub.execute_input":"2023-08-25T05:16:58.885280Z","iopub.status.idle":"2023-08-25T05:17:09.974758Z","shell.execute_reply.started":"2023-08-25T05:16:58.885237Z","shell.execute_reply":"2023-08-25T05:17:09.972940Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Image shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 0\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 2\nImage shape: torch.Size([3, 224, 224]), Label: 1\nImage shape: torch.Size([3, 224, 224]), Label: 0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# images는 배치 내 이미지들의 텐서입니다.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# labels는 배치 내 이미지들에 해당하는 클래스 레이블입니다.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 이미지와 클래스를 확인하는 예제 코드\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(images)):\n\u001b[1;32m      7\u001b[0m         image \u001b[38;5;241m=\u001b[39m images[i]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:933\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    887\u001b[0m ):\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:1200\u001b[0m, in \u001b[0;36mTiffImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_load_libtiff:\n\u001b[0;32m-> 1200\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_libtiff\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mload()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:1302\u001b[0m, in \u001b[0;36mTiffImageFile._load_libtiff\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1301\u001b[0m     \u001b[38;5;66;03m# 4 bytes, otherwise the trace might error out\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m     n, err \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfpfp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;66;03m# we have something else.\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have fileno or getvalue. just reading\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"device","metadata":{"execution":{"iopub.status.busy":"2023-08-25T05:20:39.443464Z","iopub.execute_input":"2023-08-25T05:20:39.443844Z","iopub.status.idle":"2023-08-25T05:20:39.451270Z","shell.execute_reply.started":"2023-08-25T05:20:39.443812Z","shell.execute_reply":"2023-08-25T05:20:39.450181Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"markdown","source":"각 클래스별로 1000장, 300 -> 224\n\nEpoch [1/30], Training Loss: 1.6064978356057025\nValidation Accuracy: 44.67%\nEpoch [2/30], Training Loss: 0.9817075044550794\nValidation Accuracy: 57.17%\nEpoch [3/30], Training Loss: 0.9212817150227567\nValidation Accuracy: 55.33%\nEpoch [4/30], Training Loss: 0.8652829971719296\nValidation Accuracy: 55.83%\nEpoch [5/30], Training Loss: 0.8256980187081276\nValidation Accuracy: 57.33%\nEpoch [6/30], Training Loss: 0.8006651211292186\nValidation Accuracy: 58.67%\nEpoch [7/30], Training Loss: 0.7753435576215704\nValidation Accuracy: 58.17%","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# 학습률 스케줄러 설정\nscheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5)\n\nnum_epochs=20\n\nbest_val_loss = float('inf')  # 초기값으로 무한대 설정\nbest_model_dir = '/kaggle/working'  # 모델을 저장할 경로\n\n\n# 훈련 및 검증 과정\nfor epoch in range(num_epochs):\n    start_time = time.time()  # 에포크 시작 시간\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_dataloader:\n        images, labels = images.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_dataloader)}')\n    \n    # 검증(validation) 과정\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for images, labels in val_dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.data, 1)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            loss = criterion(outputs, labels)\n            \n            val_loss += loss.item()\n            \n            \n    end_time = time.time()  # 에포크 종료 시간\n    epoch_time = end_time - start_time  # 에포크 소요 시간\n    \n    val_loss /= len(val_dataloader)\n    val_accuracy = correct / total\n    print(f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n    print(f'Validation Loss: {val_loss}, Epoch Time: {epoch_time:.2f} seconds')\n    \n    scheduler.step(val_loss)  # 학습률 스케줄러 업데이트\n    \n    \n    if val_loss < best_val_loss:\n        if epoch > 5:\n            best_val_loss = val_loss\n            best_model_path = os.path.join(best_model_dir, f'0826_10_{epoch+1}epoch.pth')\n            torch.save(model.state_dict(), best_model_path)\n            print(f'Saved best model with validation loss: {best_val_loss} at epoch {epoch+1}')\n        \nprint('Training and validation finished.')","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:53:16.786544Z","iopub.execute_input":"2023-08-26T00:53:16.786932Z","iopub.status.idle":"2023-08-26T02:13:00.615144Z","shell.execute_reply.started":"2023-08-26T00:53:16.786901Z","shell.execute_reply":"2023-08-26T02:13:00.614127Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Epoch [1/20], Training Loss: 1.0951093752505416\nValidation Accuracy: 52.53%\nValidation Loss: 0.9407660687963167, Epoch Time: 239.66 seconds\nEpoch [2/20], Training Loss: 0.9623003365629811\nValidation Accuracy: 48.80%\nValidation Loss: 0.9079927628238996, Epoch Time: 241.42 seconds\nEpoch [3/20], Training Loss: 0.9383854106321173\nValidation Accuracy: 54.13%\nValidation Loss: 0.8992436404029528, Epoch Time: 240.87 seconds\nEpoch [4/20], Training Loss: 0.9146840986558946\nValidation Accuracy: 53.87%\nValidation Loss: 0.9001684871812662, Epoch Time: 239.88 seconds\nEpoch [5/20], Training Loss: 0.9112482412386749\nValidation Accuracy: 58.13%\nValidation Loss: 0.863901416460673, Epoch Time: 241.43 seconds\nEpoch [6/20], Training Loss: 0.8980976971529298\nValidation Accuracy: 55.47%\nValidation Loss: 0.8844981342554092, Epoch Time: 239.88 seconds\nValidation Accuracy: 55.87%\nValidation Loss: 0.9120593319336573, Epoch Time: 240.99 seconds\nSaved best model with validation loss: 0.9120593319336573 at epoch 7\nEpoch [8/20], Training Loss: 0.8931057249085378\nValidation Accuracy: 56.27%\nValidation Loss: 0.8551276922225952, Epoch Time: 240.14 seconds\nSaved best model with validation loss: 0.8551276922225952 at epoch 8\nEpoch [9/20], Training Loss: 0.8883626071073241\nValidation Accuracy: 55.07%\nValidation Loss: 0.8619494140148163, Epoch Time: 238.74 seconds\nEpoch [10/20], Training Loss: 0.8721631696668722\nValidation Accuracy: 55.87%\nValidation Loss: 0.8631236180663109, Epoch Time: 239.83 seconds\nEpoch [11/20], Training Loss: 0.8765066476191504\nValidation Accuracy: 54.13%\nValidation Loss: 0.8916182344158491, Epoch Time: 238.22 seconds\nEpoch [12/20], Training Loss: 0.8695619257829957\nValidation Accuracy: 55.60%\nValidation Loss: 0.8608725046118101, Epoch Time: 238.99 seconds\nEpoch [13/20], Training Loss: 0.8638982312153962\nValidation Accuracy: 56.13%\nValidation Loss: 0.8819159430762132, Epoch Time: 237.90 seconds\nEpoch [14/20], Training Loss: 0.8533504223419448\nValidation Accuracy: 53.47%\nValidation Loss: 0.9709240843852361, Epoch Time: 237.46 seconds\nEpoch [15/20], Training Loss: 0.8201897629236771\nValidation Accuracy: 61.20%\nValidation Loss: 0.7961884985367457, Epoch Time: 237.30 seconds\nSaved best model with validation loss: 0.7961884985367457 at epoch 15\nEpoch [16/20], Training Loss: 0.8095878904148683\nValidation Accuracy: 61.20%\nValidation Loss: 0.8187295583387216, Epoch Time: 237.61 seconds\nEpoch [17/20], Training Loss: 0.8028949852717124\nValidation Accuracy: 59.33%\nValidation Loss: 0.8198782106240591, Epoch Time: 237.27 seconds\nEpoch [18/20], Training Loss: 0.8026620347621077\nValidation Accuracy: 59.20%\nValidation Loss: 0.8058174786468347, Epoch Time: 239.13 seconds\nEpoch [19/20], Training Loss: 0.8008716684276774\nValidation Accuracy: 59.07%\nValidation Loss: 0.8188791858653227, Epoch Time: 238.11 seconds\nEpoch [20/20], Training Loss: 0.7860904923940109\nValidation Accuracy: 61.60%\nValidation Loss: 0.798804797232151, Epoch Time: 238.52 seconds\nTraining and validation finished.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Validation**","metadata":{}},{"cell_type":"code","source":"model = models.efficientnet_b2()\nmodel.load_state_dict(torch.load('/kaggle/input/11epoch-oasis/best_model_epoch11.pth'))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:27:33.240501Z","iopub.execute_input":"2023-08-26T00:27:33.240862Z","iopub.status.idle":"2023-08-26T00:27:33.551012Z","shell.execute_reply.started":"2023-08-26T00:27:33.240833Z","shell.execute_reply":"2023-08-26T00:27:33.550073Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"EfficientNet(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n      )\n    )\n    (2): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n      )\n    )\n    (3): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n      )\n    )\n    (4): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n            (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(288, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(528, 528, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=528, bias=False)\n            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(528, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n      )\n    )\n    (5): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(88, 528, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(528, 528, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=528, bias=False)\n            (1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(528, 22, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(22, 528, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(528, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=720, bias=False)\n            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(720, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n      )\n    )\n    (6): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(120, 720, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(720, 720, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=720, bias=False)\n            (1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(720, 30, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(30, 720, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(720, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n      )\n      (2): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n      )\n      (3): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n      )\n      (4): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1248, 1248, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1248, bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1248, 208, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n      )\n    )\n    (7): Sequential(\n      (0): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(208, 1248, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(1248, 1248, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1248, bias=False)\n            (1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(1248, 52, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(52, 1248, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(1248, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n      )\n      (1): MBConv(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(352, 2112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(2112, 2112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2112, bias=False)\n            (1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): SiLU(inplace=True)\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(2112, 88, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(88, 2112, kernel_size=(1, 1), stride=(1, 1))\n            (activation): SiLU(inplace=True)\n            (scale_activation): Sigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(2112, 352, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n      )\n    )\n    (8): Conv2dNormActivation(\n      (0): Conv2d(352, 1408, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(1408, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): SiLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): Dropout(p=0.3, inplace=True)\n    (1): Linear(in_features=1408, out_features=1000, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def preprocess_image(image):\n        val_transform = transforms.Compose([\n        transforms.Resize((370, 370)),  # 이미지를 충분히 크게 리사이즈\n        transforms.CenterCrop(260),      # 중앙 부분을 224x224 크기로 자름\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n        return val_transform(image).unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:25:24.561323Z","iopub.execute_input":"2023-08-26T00:25:24.561693Z","iopub.status.idle":"2023-08-26T00:25:24.568506Z","shell.execute_reply.started":"2023-08-26T00:25:24.561657Z","shell.execute_reply":"2023-08-26T00:25:24.567567Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\nimage_path = '/kaggle/input/surface-crack-detection/Negative/00024.jpg'\nimage = Image.open(image_path)\n\n\n# 이미지 전처리\nmodel.to(torch.device('cpu'))\npreprocessed_image = preprocess_image(image)\npreprocessed_image = preprocessed_image.to(torch.device('cpu'))\n\n\n# 모델 추론\nwith torch.no_grad():\n    outputs = model(preprocessed_image)\n\n# 추론 결과 분석\npredicted_class = torch.argmax(outputs).item()\nclass_names = [\"class1\", \"class2\", \"class3\"]  # 클래스명을 실제로 적절히 수정하세요\npredicted_label = class_names[predicted_class]\n\nprint(f'Predicted class: {predicted_label}')\nprint(outputs)","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:40:58.132596Z","iopub.execute_input":"2023-08-26T00:40:58.132951Z","iopub.status.idle":"2023-08-26T00:40:58.239045Z","shell.execute_reply.started":"2023-08-26T00:40:58.132922Z","shell.execute_reply":"2023-08-26T00:40:58.238048Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Predicted class: class3\ntensor([[-0.4456,  0.5223,  1.0447]])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs.shape, predicted_class","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:37:49.949711Z","iopub.execute_input":"2023-08-26T00:37:49.950072Z","iopub.status.idle":"2023-08-26T00:37:49.957154Z","shell.execute_reply.started":"2023-08-26T00:37:49.950041Z","shell.execute_reply":"2023-08-26T00:37:49.956075Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(torch.Size([1, 3]), 2)"},"metadata":{}}]},{"cell_type":"code","source":"probs = torch.nn.functional.softmax(outputs[0], dim=0)\nprobs","metadata":{"execution":{"iopub.status.busy":"2023-08-26T00:38:56.947919Z","iopub.execute_input":"2023-08-26T00:38:56.948333Z","iopub.status.idle":"2023-08-26T00:38:56.956777Z","shell.execute_reply.started":"2023-08-26T00:38:56.948301Z","shell.execute_reply":"2023-08-26T00:38:56.955775Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([0.1624, 0.3278, 0.5098])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"클래스별 2000장 + LR 스케줄러\n\nEpoch [1/30], Training Loss: 1.2952196319052514\nValidation Accuracy: 53.50%\nEpoch [2/30], Training Loss: 0.9428603240150086\nValidation Accuracy: 56.33%\nEpoch [3/30], Training Loss: 0.8739769541202708\nValidation Accuracy: 55.17%\nEpoch [4/30], Training Loss: 0.8027256749411846\nValidation Accuracy: 62.33%\nEpoch [5/30], Training Loss: 0.7732083461386092\nValidation Accuracy: 63.50%\nEpoch [6/30], Training Loss: 0.7489838609670071\nValidation Accuracy: 65.33%\nEpoch [7/30], Training Loss: 0.7324732732265553\nValidation Accuracy: 66.50%\nEpoch [8/30], Training Loss: 0.7263451226531191\nValidation Accuracy: 65.67%\nEpoch [9/30], Training Loss: 0.7261482638881561\nValidation Accuracy: 67.83%\nEpoch [10/30], Training Loss: 0.7206828971492484\nValidation Accuracy: 66.50%\nEpoch [11/30], Training Loss: 0.7176627115683353\nValidation Accuracy: 65.33%\nEpoch [12/30], Training Loss: 0.7192867640802201\nValidation Accuracy: 66.17%","metadata":{}},{"cell_type":"markdown","source":"클래스별 2500장 + 플루토\n\nEpoch [1/30], Training Loss: 1.1889205927544453\nValidation Accuracy: 54.17%\nEpoch [2/30], Training Loss: 0.9099420851849495\nValidation Accuracy: 62.83%\nEpoch [3/30], Training Loss: 0.843723376253818\nValidation Accuracy: 55.33%\nEpoch [4/30], Training Loss: 0.809394041528093\nValidation Accuracy: 64.33%\nEpoch [5/30], Training Loss: 0.7885438896240072\nValidation Accuracy: 67.00%\nEpoch [6/30], Training Loss: 0.7733952780987354\nValidation Accuracy: 68.67%\nEpoch [7/30], Training Loss: 0.7437666062344896\nValidation Accuracy: 63.67%\nEpoch [8/30], Training Loss: 0.7247865476506822\nValidation Accuracy: 68.67%\nEpoch [9/30], Training Loss: 0.7039683355930003\nValidation Accuracy: 67.83%\nEpoch [10/30], Training Loss: 0.6979192223954708\nValidation Accuracy: 64.33%\nEpoch [11/30], Training Loss: 0.675495804751173\nValidation Accuracy: 68.50%\nEpoch [12/30], Training Loss: 0.6578394636194757\nValidation Accuracy: 64.83%\nEpoch [13/30], Training Loss: 0.6409801398185974\nValidation Accuracy: 64.00%","metadata":{}},{"cell_type":"markdown","source":"Epoch [1/30], Training Loss: 0.9065040145890187\nValidation Accuracy: 53.33%\nValidation Loss: 0.967835816897844\nSaved best model with validation loss: 0.967835816897844 at epoch 1\nEpoch [2/30], Training Loss: 0.8331992423130294\nValidation Accuracy: 61.50%\nValidation Loss: 0.8213111742546684\nSaved best model with validation loss: 0.8213111742546684 at epoch 2\nEpoch [3/30], Training Loss: 0.8021890476598578\nValidation Accuracy: 61.83%\nValidation Loss: 0.79155286048588\nSaved best model with validation loss: 0.79155286048588 at epoch 3\nEpoch [4/30], Training Loss: 0.7564150370783725\nValidation Accuracy: 65.17%\nValidation Loss: 0.797015550889467\nEpoch [5/30], Training Loss: 0.7408542172383454\nValidation Accuracy: 63.33%\nValidation Loss: 0.8201270370106948\nEpoch [6/30], Training Loss: 0.731644096212872\nValidation Accuracy: 67.50%\nValidation Loss: 0.7349986355555685\nSaved best model with validation loss: 0.7349986355555685 at epoch 6\nEpoch [7/30], Training Loss: 0.7122625491376651\nValidation Accuracy: 61.17%\nValidation Loss: 0.8313404886346114\nEpoch [8/30], Training Loss: 0.7023708483930361\nValidation Accuracy: 61.50%\nValidation Loss: 0.8536729185204757\nEpoch [9/30], Training Loss: 0.6878132095781423\nValidation Accuracy: 62.50%\nValidation Loss: 0.7526584322515287\nEpoch [10/30], Training Loss: 0.6767833698604067\nValidation Accuracy: 62.50%\nValidation Loss: 0.859652149834131\nEpoch [11/30], Training Loss: 0.6659834929442001\nValidation Accuracy: 65.83%\nValidation Loss: 0.7337929873090041\nSaved best model with validation loss: 0.7337929873090041 at epoch 11\nEpoch [12/30], Training Loss: 0.6564294833247944\nValidation Accuracy: 66.50%\nValidation Loss: 0.737569195659537\nEpoch [13/30], Training Loss: 0.6479163605277821\nValidation Accuracy: 66.67%\nValidation Loss: 0.7145659076540094\nSaved best model with validation loss: 0.7145659076540094 at epoch 13\nEpoch [14/30], Training Loss: 0.6386325454307815\nValidation Accuracy: 68.33%\nValidation Loss: 0.7163053766677254\nEpoch [15/30], Training Loss: 0.6163983970375384\nValidation Accuracy: 67.33%\nValidation Loss: 0.7446301140283283\nEpoch [16/30], Training Loss: 0.6113806268926394\nValidation Accuracy: 65.67%\nValidation Loss: 0.7817334854289105\nEpoch [17/30], Training Loss: 0.5944197480961427\nValidation Accuracy: 67.83%\nValidation Loss: 0.7560262413401353\nEpoch [18/30], Training Loss: 0.5794706932568954\nValidation Accuracy: 67.33%\nValidation Loss: 0.791797548532486\nEpoch [19/30], Training Loss: 0.5588591801918159\nValidation Accuracy: 67.50%\nValidation Loss: 0.7283502283849215\nEpoch [20/30], Training Loss: 0.47411285062967723\nValidation Accuracy: 67.33%\nValidation Loss: 0.8110448561216655\nEpoch [21/30], Training Loss: 0.43115679773233706\nValidation Accuracy: 68.50%\nValidation Loss: 0.8360281853299392\nEpoch [22/30], Training Loss: 0.41830782743833833\nValidation Accuracy: 66.50%\nValidation Loss: 0.8703461226664091\nEpoch [23/30], Training Loss: 0.3992644193819014\nValidation Accuracy: 65.00%\nValidation Loss: 0.9392865096267901\nEpoch [24/30], Training Loss: 0.38263551423610265\nValidation Accuracy: 66.67%\nValidation Loss: 0.9623668962403348\nEpoch [25/30], Training Loss: 0.35678213362471534\nValidation Accuracy: 61.50%\nValidation Loss: 1.1999907760243667\nEpoch [26/30], Training Loss: 0.3435413336097184\nValidation Accuracy: 65.83%\nValidation Loss: 0.9565253806741614","metadata":{}},{"cell_type":"markdown","source":"transforms.RandomRotation(degrees=15),  # 무작위 회전 (±15도)\n    transforms.RandomHorizontalFlip(),  # 무작위 수평 반전\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # 색상 변형\n    \n    증강 추가하니까 성능 더 구려짐 ;\n\n\nEpoch [1/20], Training Loss: 1.0951093752505416\nValidation Accuracy: 52.53%\nValidation Loss: 0.9407660687963167, Epoch Time: 239.66 seconds\nEpoch [2/20], Training Loss: 0.9623003365629811\nValidation Accuracy: 48.80%\nValidation Loss: 0.9079927628238996, Epoch Time: 241.42 seconds\nEpoch [3/20], Training Loss: 0.9383854106321173\nValidation Accuracy: 54.13%\nValidation Loss: 0.8992436404029528, Epoch Time: 240.87 seconds\nEpoch [4/20], Training Loss: 0.9146840986558946\nValidation Accuracy: 53.87%\nValidation Loss: 0.9001684871812662, Epoch Time: 239.88 seconds\nEpoch [5/20], Training Loss: 0.9112482412386749\nValidation Accuracy: 58.13%\nValidation Loss: 0.863901416460673, Epoch Time: 241.43 seconds\nEpoch [6/20], Training Loss: 0.8980976971529298\nValidation Accuracy: 55.47%\nValidation Loss: 0.8844981342554092, Epoch Time: 239.88 seconds\nValidation Accuracy: 55.87%\nValidation Loss: 0.9120593319336573, Epoch Time: 240.99 seconds\nSaved best model with validation loss: 0.9120593319336573 at epoch 7\nEpoch [8/20], Training Loss: 0.8931057249085378\nValidation Accuracy: 56.27%\nValidation Loss: 0.8551276922225952, Epoch Time: 240.14 seconds\nSaved best model with validation loss: 0.8551276922225952 at epoch 8\nEpoch [9/20], Training Loss: 0.8883626071073241\nValidation Accuracy: 55.07%\nValidation Loss: 0.8619494140148163, Epoch Time: 238.74 seconds\nEpoch [10/20], Training Loss: 0.8721631696668722\nValidation Accuracy: 55.87%\nValidation Loss: 0.8631236180663109, Epoch Time: 239.83 seconds\nEpoch [11/20], Training Loss: 0.8765066476191504\nValidation Accuracy: 54.13%\nValidation Loss: 0.8916182344158491, Epoch Time: 238.22 seconds\nEpoch [12/20], Training Loss: 0.8695619257829957\nValidation Accuracy: 55.60%\nValidation Loss: 0.8608725046118101, Epoch Time: 238.99 seconds\nEpoch [13/20], Training Loss: 0.8638982312153962\nValidation Accuracy: 56.13%\nValidation Loss: 0.8819159430762132, Epoch Time: 237.90 seconds\nEpoch [14/20], Training Loss: 0.8533504223419448\nValidation Accuracy: 53.47%\nValidation Loss: 0.9709240843852361, Epoch Time: 237.46 seconds\nEpoch [15/20], Training Loss: 0.8201897629236771\nValidation Accuracy: 61.20%\nValidation Loss: 0.7961884985367457, Epoch Time: 237.30 seconds\nSaved best model with validation loss: 0.7961884985367457 at epoch 15\nEpoch [16/20], Training Loss: 0.8095878904148683\nValidation Accuracy: 61.20%\nValidation Loss: 0.8187295583387216, Epoch Time: 237.61 seconds\nEpoch [17/20], Training Loss: 0.8028949852717124\nValidation Accuracy: 59.33%\nValidation Loss: 0.8198782106240591, Epoch Time: 237.27 seconds\nEpoch [18/20], Training Loss: 0.8026620347621077\nValidation Accuracy: 59.20%\nValidation Loss: 0.8058174786468347, Epoch Time: 239.13 seconds\nEpoch [19/20], Training Loss: 0.8008716684276774\nValidation Accuracy: 59.07%\nValidation Loss: 0.8188791858653227, Epoch Time: 238.11 seconds\nEpoch [20/20], Training Loss: 0.7860904923940109\nValidation Accuracy: 61.60%\nValidation Loss: 0.798804797232151, Epoch Time: 238.52 seconds\nTraining and validation finished.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}